\documentclass[12pt]{report}
\linespread{2}
\usepackage[pdftex]{graphicx}   
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{stackengine}
\usepackage[margin=1.25in]{geometry}
% latex
\usepackage[hidelinks]{hyperref}
%\usepackage{hyperref}
\usepackage[hyphenbreaks]{breakurl}




%opening
\title{Unsupervised Out of Vocabulary Word Handling for Neural Machine Translation.}
\author{Raja Gunasekaran}

\usepackage{graphicx}
\graphicspath{ {images/} }

\begin{document}

\maketitle

%\begin{abstract}
%
%\end{abstract}

\tableofcontents
%\listoffigures
%\listoftables

\chapter{Introduction}
\input{tex/intro.tex}

\chapter{Literature Survey}
\input{tex/background.tex}

\chapter{Related Work}
\input{tex/related.tex}

\chapter{Related Work}
\input{tex/proposed.tex}

\chapter{Experiments and Evaluation}
\input{tex/experiments.tex}

\chapter{Conclusion and Future Work}
\input{tex/conclusion.tex}

%\input{tex/timeline.tex}
%\subsubsection{Candidate Rule Extraction}
%For every word pair $(w_1 , w_2 )$ in vocabulary $V$, all possible prefix and suffix substitutions from $w_1$ to $w_2$ of predefined length 6 are extracted. For each rule $r$, its support set $S_r$ - set of all word pair that obey rule $r$, is extracted.
%
%\[ S_r = \{(w_1 , w_2 ) \in V^2 |w_1 \xrightarrow{r} w_2 \} \]
%
%
%\subsubsection{Evaluate candidate rules}
%The hit rate for the rules is computed using cosine rank. For every word-pair combination in $S_r \times S_r$, we can calculate the percent of the time similairity rank is higher than the threshold $t_{rank}$. For word pair $((walking, walk),(talking, talk))$, the rank of $(walking, walk + \uparrow d_{talk\_\_})$ was computed.  It can be seen that meaning preserving rules like $suffix:ed:ing$ receive high hit rate while non meaning preserving rule receive low hit rate.
%
%
%
%\subsubsection{Generate Morphological Transformation}
%
%
%For each rule $r$, the best direction vector $\uparrow{d_w}$ is computed greedily. The direction vector $\uparrow{d_w}$ which explains most of the word pair in support is selected recursively for all unexplained word pairs. By doing this, we will have the morphological transformations for each rule. These morphological transformations are subject to further evaluation using cosine similarity and cosine similarity rank. The cosine similarity threshold was set to $t_{cosine}$ = 0.5 and cosine rank threshold was set to $t_{rank}$ = 30. If these thresholds are not met, the rules are removed.
%
%
%The morphological transformations also have a graph based interpretations. A labelled, weighted,	, directed multi-graph $G^V_{Morph}$ is created from the transformations where nodes are the words in vocabulary and edges are the morphological transformation with $(rank, cosine)$ as the weights. From all the candidate morphological transformations 1-1 mappings are selected based on the the following conditions.
%
%\begin{itemize}
%	\item Edges are considered only if they start from higher count word and end at lower count word.
%	\item If multiple edges exist, the ones with minimal rank $rank$ is selected.
%	\item If multiple edges still exist, the one with maximum $cosine$ is selected.
%\end{itemize}
%
%This normalizes the graph to weighted directed graph $D^V_{Morph}$ where low occurring words are mapped to high occurring words using a direction vector.
%
%
%\subsubsection{Mapping Rare and Unknown Words}
%The word vectors for out of vocabulary words, and low frequency words can generated by mapping them to nodes in $D^V_{Morph}$ using the morphological rules. $V_c$ is the set of all low frequency words and OOV words.
%
%\[ V_c=\{w \in V|C\leq count(w)\} \]
%
%All the morphological rule $r\in R$ are applied on words $w\in V_c$ and if any of them results in a word $w'\in D^V_{Morph}$ then that word is mapped into the graph using the direction vector for rule $r$. This allows us to get meaning full representation for Rare and Unknown words at testing time.
%


% SUmmarize key points. less emphasis on technical grounds. Put in exact technical content.

\bibliography{bib}
\bibliographystyle{apalike}


\end{document}
