\section{Related Works}
Pioneering works in NMT like \cite{sutskever2014sequence} and \cite{bahdanau2014neural} observed that sentences with rare and unknown words often produced poor translations when compared to sentences with many frequent words. Although problem of handling unknown and rare words were focused for generating word embedding, it was not addressed in any of the early works in NMT\citep{luong2015addressing}. 

To get vector representations for rare and unknown words,  morphological and orthographic information can be used\citep{botha2014compositional,luong2013better,bhatia2016morphological} while training. Another approach is to get embeddings for characters or character n-grams by breaking down words \citep{bojanowski2016enriching,kim2016character,wieting2016charagram}. This allows us to get words embeddings for any words by convoluting over character embeddings.

%Since 2013, there have been many techniques proposed for NMT to get good vector representations for rare and unknown words. 


Specific to NMT, \cite{jean2014using} proposed a method based on importance sampling to use very large target vocabulary without increasing the complexity of the NMT system. In their attention based NMT, the attention weights were used to determine the alignment of unknown (\textit{unk}) target words with the source word, usually a rare word, in the translation. Then a dictionary is used to replace the \textit{unk} tokens with the translation of the rare source word.

\cite{luong2015addressing} proposed a similar model using external aligner instead of using the attention mechanism. An external aligner was used to align words from the source sentence and the generated target sentence. During training, for all the unknown source word, the aligned target word is replaced with a unknown token \textit{unk}. In their copy model, each unknown target word is assigned individual \textit{unk} token based on their source word. The alignment between source words and target words are maintained. In their positional model, a pre-build dictionary is used to replace the unknown source words in the target side based on the alignment. Both these approaches were effective and showed performance comparable or better than the state-of-the-art in English-French and English-German language pairs. \cite{choi2017context} extended the work of \cite{luong2015addressing} to include multiple positional unknown tokens for digits, proper noun and acronym instead of just one \textit{unk} token. 

One problem with these dictionary back-off methods is that there is not always 1-1 correspondence between words from different language because of the variance in degree of morphological synthesis between languages. \cite{sennrich2015neural} proposed a system that work on subword unit level instead of word level like the previous models. The words are segmented into subword units using Byte Pair Encoding (BPE) \citep{gage1994new}. BPE is data compression techniques where the most frequency character or character sequences are iteratively replaced with unused bytes. The vocabulary of their NMT system comprises entirely of these subword units of different lengths. They demonstrated that subword models achieve better accuracy in translating rare words. The model was able to generate new words unseen during training time and improved En-De and En-Ru translation over other back-off dictionary models. One of the major contribution of the paper was showing that the NMT systems are capable of achieving open vocabulary translation by modelling sub word units.

Similar to \cite{sennrich2015neural}, \cite{luong2016achieving} presented a open vocabulary NMT system based on word and character embedding models using RNN. Their hybrid model translates at word level and falls back to character components for rare words. The representation for rare words are computed using Recurrent Neural Network working on the character level. Their system is faster and easier to train unlike other NMT systems using character and subword units. They produced state-of-the-art for English-Czech translation on WMT'15 dataset.


